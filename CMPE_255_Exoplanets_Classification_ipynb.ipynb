{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Shashanksai6/255_finalproject/blob/main/CMPE_255_Exoplanets_Classification_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubu8NV4r-UKD"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yp0eKFUn95Qy"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5-qDyLj99_hW",
    "outputId": "5d57ef51-6978-48a0-d3bc-24993fb168da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting MarkupSafe==2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Installing collected packages: MarkupSafe\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.2\n",
      "    Uninstalling MarkupSafe-2.1.2:\n",
      "      Successfully uninstalled MarkupSafe-2.1.2\n",
      "Successfully installed MarkupSafe-2.1.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.65.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.5.3)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.7.5)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.2.2)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (3.3.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.22.4)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (0.40.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lazypredict) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->lazypredict) (1.16.0)\n",
      "Installing collected packages: lazypredict\n",
      "Successfully installed lazypredict-0.2.12\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ydata-profiling\n",
      "  Downloading ydata_profiling-4.1.2-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.9/345.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: seaborn<0.13,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (0.12.2)\n",
      "Collecting htmlmin==0.1.12\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting imagehash==4.3.1\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting phik<0.13,>=0.11.1\n",
      "  Downloading phik-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (679 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.5/679.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.24,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (1.22.4)\n",
      "Collecting matplotlib<3.7,>=3.2\n",
      "  Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multimethod<1.10,>=1.4\n",
      "  Downloading multimethod-1.9.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (3.1.2)\n",
      "Requirement already satisfied: pandas!=1.4.0,<1.6,>1.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (1.5.3)\n",
      "Collecting visions[type_image_path]==0.7.5\n",
      "  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<2.29,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (2.27.1)\n",
      "Requirement already satisfied: statsmodels<0.14,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (0.13.5)\n",
      "Collecting scipy<1.10,>=1.4.1\n",
      "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typeguard<2.14,>=2.13.2\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (6.0)\n",
      "Collecting tqdm<4.65,>=4.48.2\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<1.11,>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (1.10.7)\n",
      "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (1.4.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (8.4.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (3.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling->pandas-profiling) (23.1.0)\n",
      "Collecting tangled-up-in-unicode>=0.0.4\n",
      "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=2.11.1->ydata-profiling->pandas-profiling) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (23.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (4.39.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling->pandas-profiling) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.4.0,<1.6,>1.1->ydata-profiling->pandas-profiling) (2022.7.1)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from phik<0.13,>=0.11.1->ydata-profiling->pandas-profiling) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<1.11,>=1.8.1->ydata-profiling->pandas-profiling) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling->pandas-profiling) (2.0.12)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling->pandas-profiling) (0.5.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels<0.14,>=0.13.2->ydata-profiling->pandas-profiling) (1.16.0)\n",
      "Building wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27096 sha256=e8d4e81aa829abc45778d254407d9742ad37fb8bf8d4046c2c0d0919e2a418e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/91/29/a79cecb328d01739e64017b6fb9a1ab9d8cb1853098ec5966d\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: htmlmin, typeguard, tqdm, tangled-up-in-unicode, scipy, multimethod, matplotlib, imagehash, visions, phik, ydata-profiling, pandas-profiling\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "Successfully installed htmlmin-0.1.12 imagehash-4.3.1 matplotlib-3.6.3 multimethod-1.9.1 pandas-profiling-3.6.6 phik-0.12.3 scipy-1.9.3 tangled-up-in-unicode-0.2.0 tqdm-4.64.1 typeguard-2.13.3 visions-0.7.5 ydata-profiling-4.1.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Installing dependencies\n",
    "!pip install MarkupSafe==2.1.1 \n",
    "!pip install lazypredict\n",
    "!pip install -U pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrRAzuDI-IF9"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn Packages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Sklearn Evaluation Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Exploratory Data Analysis (EDA) \n",
    "from pandas_profiling import ProfileReport\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QdqZ8lI-pCM"
   },
   "source": [
    "# Project Objective:\n",
    "\n",
    "In this project, our goal is to build a model that can predict the existence of an exoplanet (i.e. a planet that orbits a distant star system) given the light intensity readings from that star over time. The dataset we’ll be using comes from NASA’s Kepler telescope currently in space. This project will demonstrate how predictive classication modeling will helps to discover does planat is exoplanate or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAWLQXHC-qVu"
   },
   "source": [
    "## Data Extraction\n",
    "\n",
    "Data extraction is the process of acquiring and processing raw data of various forms and types to improve the operational paradigms of an organization.\n",
    "\n",
    "It is perhaps the most important operation of the Extract/Transform/Load (ETL) process because it is the foundation for critical analyses and  decision making processes. It enables consolidation, analysis and refining of data so that it can be converted into meaningful information that can be stored for further use and manipulation. The extracted data can help in decision making, customer base expansion, service improvements, predicting sales and optimizing costs, among other things.\n",
    "\n",
    "In our use case, we are using NASA-Caltech API (https://exoplanetarchive.ipac.caltech.edu/docs/program_interfaces.html) to retrive the information captured by Kaper telescope. We transformed the JSON data from API to CSV using Excel to make it available for Machine Learning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPVxQS76-xZz"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "df = pd.read_csv('/content/drive/My Drive/dataset/exoplanet/exoplanets_nasa.csv')\n",
    "df = pd.DataFrame(np.repeat(df.values, 25, axis=0))\n",
    "\n",
    "df.head(2)\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwjNIH8k_Xh2"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ss9Ue77A_bo_"
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df, html={'style':{'full_width':True}})\n",
    "#profile.to_notebook_iframe()\n",
    "profile.to_file(\"/content/drive/My Drive/dataset/expanded_exoplanets_profile_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Feature Engineering and Transformation\n",
    "\n",
    "Feature engineering is the process of transforming features, extracting features, and creating new variables from the original data, to train machine learning models.\n",
    "\n",
    "Data in its original format can almost never be used straightaway to train classification or regression models. Instead, data scientists devote a huge chunk of their time to data preprocessing to train machine learning algorithms. Feature engineering is key to improving the performance of machine learning algorithms. Yet, it is very time-consuming. Fortunately, there are many Python libraries that we can use for data preparation.\n",
    "\n",
    "Some techniques above might work better with some algorithms or datasets, while some of them might be beneficial in all casses. Based on current situtation, There are couple of transformation takes place on data based followed as:\n",
    "\n",
    "- Change the columns name\n",
    "- Dropped some columns\n",
    "- Transformed target variables\n",
    "- Handling Missing values\n",
    "- Remove Duplicate instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Change the columns name\n",
    "df = df.rename(columns={'kepid': 'KepID',\n",
    "                        'kepoi_name': 'KOIName',\n",
    "                        'kepler_name': 'KeplerName',\n",
    "                        'koi_disposition': 'ExoplanetArchiveDisposition',\n",
    "                        'koi_pdisposition': 'DispositionUsingKeplerData',\n",
    "                        'koi_score': 'DispositionScore',\n",
    "                        'koi_fpflag_nt': 'NotTransit-LikeFalsePositiveFlag',\n",
    "                        'koi_fpflag_ss': 'koi_fpflag_ss',\n",
    "                        'koi_fpflag_co': 'CentroidOffsetFalsePositiveFlag',\n",
    "                        'koi_fpflag_ec': 'EphemerisMatchIndicatesContaminationFalsePositiveFlag',\n",
    "                        'koi_period': 'OrbitalPeriod.days',\n",
    "                        'koi_period_err1': 'OrbitalPeriodUpperUnc.days',\n",
    "                        'koi_period_err2': 'OrbitalPeriodLowerUnc.days',\n",
    "                        'koi_time0bk': 'TransitEpoch.BKJD',\n",
    "                        'koi_time0bk_err1': 'TransitEpochUpperUnc.BKJD',\n",
    "                        'koi_time0bk_err2': 'TransitEpochLowerUnc.BKJD',\n",
    "                        'koi_impact': 'ImpactParamete',\n",
    "                        'koi_impact_err1': 'ImpactParameterUpperUnc',\n",
    "                        'koi_impact_err2': 'ImpactParameterLowerUnc',\n",
    "                        'koi_duration': 'TransitDuration.hrs',\n",
    "                        'koi_duration_err1': 'TransitDurationUpperUnc.hrs',\n",
    "                        'koi_duration_err2': 'TransitDurationLowerUnc.hrs',\n",
    "                        'koi_depth': 'TransitDepth.ppm',\n",
    "                        'koi_depth_err1': 'TransitDepthUpperUnc.ppm',\n",
    "                        'koi_depth_err2': 'TransitDepthLowerUnc.ppm',\n",
    "                        'koi_prad': 'PlanetaryRadius.Earthradii',\n",
    "                        'koi_prad_err1': 'PlanetaryRadiusUpperUnc.Earthradii',\n",
    "                        'koi_prad_err2': 'PlanetaryRadiusLowerUnc.Earthradii',\n",
    "                        'koi_teq': 'EquilibriumTemperature.K',\n",
    "                        'koi_teq_err1': 'EquilibriumTemperatureUpperUnc.K',\n",
    "                        'koi_teq_err2': 'EquilibriumTemperatureLowerUnc.K',\n",
    "                        'koi_insol': 'InsolationFlux.Earthflux',\n",
    "                        'koi_insol_err1': 'InsolationFluxUpperUnc.Earthflux',\n",
    "                        'koi_insol_err2': 'InsolationFluxLowerUnc.Earthflux',\n",
    "                        'koi_model_snr': 'TransitSignal-to-Nois',\n",
    "                        'koi_tce_plnt_num': 'TCEPlanetNumbe',\n",
    "                        'koi_tce_delivname': 'TCEDeliver',\n",
    "                        'koi_steff': 'StellarEffectiveTemperature.K',\n",
    "                        'koi_steff_err1': 'StellarEffectiveTemperatureUpperUnc.K',\n",
    "                        'koi_steff_err2': 'StellarEffectiveTemperatureLowerUnc.K',\n",
    "                        'koi_slogg': 'StellarSurfaceGravity.log10(cm/s**2)',\n",
    "                        'koi_slogg_err1': 'StellarSurfaceGravityUpperUnc.log10(cm/s**2)',\n",
    "                        'koi_slogg_err2': 'StellarSurfaceGravityLowerUnc.log10(cm/s**2)',\n",
    "                        'koi_srad': 'StellarRadius.Solarradii',\n",
    "                        'koi_srad_err1': 'StellarRadiusUpperUnc.Solarradii',\n",
    "                        'koi_srad_err2': 'StellarRadiusLowerUnc.Solarradii',\n",
    "                        'ra': 'RA.decimaldegrees',\n",
    "                        'dec': 'Decdecimaldegrees',\n",
    "                        'koi_kepmag': 'Kepler-band.mag'\n",
    "                        })\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2. Transformed target variables\n",
    "df['ExoplanetCandidate'] = df['DispositionUsingKeplerData'].apply(lambda x: 1 if x == 'CANDIDATE' else 0)\n",
    "#df['ExoplanetConfirmed'] = df['ExoplanetArchiveDisposition'].apply(lambda x: 2 if x == 'CONFIRMED' else 1 if x == 'CANDIDATE' else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3. Dropped some columns\n",
    "df.drop(columns=['KeplerName', 'KOIName', 'EquilibriumTemperatureUpperUnc.K',\n",
    "                 'KepID', 'ExoplanetArchiveDisposition', 'DispositionUsingKeplerData',\n",
    "                 'NotTransit-LikeFalsePositiveFlag', 'koi_fpflag_ss', 'CentroidOffsetFalsePositiveFlag',\n",
    "                 'EphemerisMatchIndicatesContaminationFalsePositiveFlag', 'TCEDeliver',\n",
    "                 'EquilibriumTemperatureLowerUnc.K'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#4. Handling Missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5. Remove Duplicate instances\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling of Data for Training and Testing Models\n",
    "Generally data used here is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model’s prediction on this subset.\n",
    "\n",
    "To keep thing ideal, We allocate 80% of total data to training and rest of 20% of data for testing.\n",
    "\n",
    "Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=['ExoplanetCandidate'])\n",
    "target = df.ExoplanetCandidate\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1, test_size=.20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Benchmark and Hyper-parameter Tunning\n",
    "The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance, just as we might turn the knobs of an AM radio to get a clear signal. When creating a machine learning model, we will be presented with design choices as to how to define your model architecture. Often, we don't immediately know what the optimal model architecture should be for a given model, and thus we would like to be able to explore a range of possibilities. In a true machine learning fashion, we will ideally ask the machine to perform this exploration and select the optimal model architecture automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Benchmark\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100%|██████████| 29/29 [00:23<00:00,  1.22it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                           Accuracy\t   Balanced Accuracy\t   ROC AUC\t       F1 Score\t      Time Taken\n",
    "Model\t\n",
    "XGBClassifier\t              0.96\t             0.96\t         0.96\t          0.96\t          1.48\n",
    "RandomForestClassifier\t      0.96\t             0.96\t         0.96\t          0.96\t          2.95\n",
    "ExtraTreesClassifier\t      0.96\t             0.96\t         0.96\t          0.96\t          0.74\n",
    "LGBMClassifier\t              0.96\t             0.96\t         0.96\t          0.96\t          1.02\n",
    "LogisticRegression\t          0.96\t             0.96\t         0.96\t          0.96\t          0.20\n",
    "BaggingClassifier\t          0.95\t             0.96\t         0.96\t          0.95\t          2.11\n",
    "AdaBoostClassifier\t          0.95\t             0.95\t         0.95\t          0.95\t          1.41\n",
    "CalibratedClassifierCV\t      0.95\t             0.95\t         0.95\t          0.95\t          2.43\n",
    "LinearSVC\t                  0.95\t             0.95\t         0.95\t          0.95\t          0.72\n",
    "SGDClassifier\t              0.95\t             0.95\t         0.95\t          0.95\t          0.15\n",
    "KNeighborsClassifier\t      0.95\t             0.95\t         0.95\t          0.95\t          0.35\n",
    "SVC\t                          0.95\t             0.95\t         0.95\t          0.95\t          0.72\n",
    "PassiveAggressiveClassifier\t  0.95\t             0.95\t         0.95\t          0.95\t          0.04\n",
    "LinearDiscriminantAnalysis\t  0.95\t             0.95\t         0.95\t          0.95\t          0.14\n",
    "RidgeClassifier\t              0.95\t             0.95\t         0.95\t          0.95\t          0.04\n",
    "RidgeClassifierCV\t          0.95\t             0.95\t         0.95\t          0.95\t          0.06\n",
    "NuSVC\t                      0.94\t             0.94\t         0.94\t          0.94\t          2.69\n",
    "ExtraTreeClassifier\t          0.94\t             0.94\t         0.94\t          0.94\t          0.03\n",
    "DecisionTreeClassifier\t      0.93\t             0.93\t         0.93\t          0.93\t          0.45\n",
    "LabelSpreading\t              0.93\t             0.93\t         0.93\t          0.93\t          3.33\n",
    "LabelPropagation\t          0.93\t             0.93\t         0.93\t          0.93\t          2.32\n",
    "Perceptron\t                  0.92\t             0.92\t         0.92\t          0.92\t          0.05\n",
    "NearestCentroid\t              0.91\t             0.91\t         0.91\t          0.91\t          0.04\n",
    "BernoulliNB\t                  0.89\t             0.89\t         0.89\t          0.89\t          0.04\n",
    "QuadraticDiscriminantAnalysis 0.77\t             0.76\t         0.76\t          0.76\t          0.07\n",
    "GaussianNB\t                  0.74\t             0.73\t         0.73\t          0.72\t          0.03\n",
    "DummyClassifier\t              0.52\t             0.50\t         0.50\t          0.36\t          0.02\n",
    "Random searching of hyperparameters\n",
    "Random search provide a discrete set of values to explore for each hyperparameter; rather providing a statistical distribution for each hyperparameter from which values may be randomly sampled.\n",
    "\n",
    "Conceptually, we’ll define a sampling distribution for each hyperparameter. we can also define how many iterations we would like to build when searching for the optimal model. For each iteration, the hyperparameter values of the model will be set by sampling the defined distributions. One of the primary theoretical backings to motivate the use of a random search for most cases, hyperparameters are not equally important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter Tunning\n",
    "\n",
    "params = {\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"max_depth\": randint(2, 6), # default 3\n",
    "    \"n_estimators\": randint(100, 150), # default 100\n",
    "    \"subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n",
    "\n",
    "search_model = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=100, cv=3, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "search_model.fit(X_train, y_train)\n",
    "\n",
    "best_model = search_model.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 100 candidates, totalling 300 fits"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
